{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Bank Competition - Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Jupyter Notebook in VS Code\n",
    "\n",
    "From directory with virtual environment in it, use this terminal command to use Jupyter Notebook in VS Code:\n",
    "\n",
    "jupyter notebook --NotebookApp.allow_origin='*' --NotebookApp.ip='0.0.0.0'\n",
    "\n",
    "### Select Python Interpreter\n",
    "\n",
    "Open Command Palette as follows: Ctrl+Shift+P\n",
    "\n",
    "Then select from VS Code Command Palette: \"Python: Select Interpreter\" and choose the same Python interpreter that you used to install Jupyter.\n",
    "\n",
    "If server not shown, enter server manually as follows: \n",
    "http://localhost:8888\n",
    "\n",
    "Reminder:  pw: \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from stepbystep_v0 import StepByStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification Problem based on Bank Customer Marketing Call Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  age          job  marital  education default  balance housing loan  \\\n",
      "0   0   42   technician  married  secondary      no        7      no   no   \n",
      "1   1   38  blue-collar  married  secondary      no      514      no   no   \n",
      "2   2   36  blue-collar  married  secondary      no      602     yes   no   \n",
      "3   3   27      student   single  secondary      no       34     yes   no   \n",
      "4   4   26   technician  married  secondary      no      889     yes   no   \n",
      "\n",
      "    contact  day month  duration  campaign  pdays  previous poutcome  y  \n",
      "0  cellular   25   aug       117         3     -1         0  unknown  0  \n",
      "1   unknown   18   jun       185         1     -1         0  unknown  0  \n",
      "2   unknown   14   may       111         2     -1         0  unknown  0  \n",
      "3   unknown   28   may        10         2     -1         0  unknown  0  \n",
      "4  cellular    3   feb       902         1     -1         0  unknown  1  \n"
     ]
    }
   ],
   "source": [
    "# Import data from .csv files\n",
    "raw_train_data = pd.read_csv('binary_bank_comp/dataset/train.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(raw_train_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (675000, 16) (675000,)\n",
      "Validation set shape: (75000, 16) (75000,)\n",
      "Training set preview:\n",
      "        age          job   marital  education default  balance housing loan  \\\n",
      "114614   34       admin.  divorced  secondary      no        0     yes   no   \n",
      "581446   35  blue-collar   married  secondary      no      622     yes   no   \n",
      "144124   37   management   married   tertiary      no       16      no   no   \n",
      "715271   45   management   married   tertiary      no     8897      no   no   \n",
      "226111   41   management    single   tertiary      no      491      no   no   \n",
      "\n",
      "         contact  day month  duration  campaign  pdays  previous poutcome  \n",
      "114614  cellular   13   may       574         3    176         2    other  \n",
      "581446  cellular   17   apr        91         1     -1         0  unknown  \n",
      "144124  cellular    9   feb       126         4     -1         0  unknown  \n",
      "715271  cellular   10   sep       215         1    185         7  failure  \n",
      "226111  cellular   13   aug       119         4     -1         0  unknown  \n",
      "114614    0\n",
      "581446    0\n",
      "144124    0\n",
      "715271    1\n",
      "226111    0\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove id column\n",
    "raw_train_data.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X = raw_train_data.drop(columns=['y'])\n",
    "y = raw_train_data['y']\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "\n",
    "# Print the first few rows of the training set\n",
    "print(\"Training set preview:\")\n",
    "print(X_train.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             age   balance       day  duration  campaign     pdays  previous  \\\n",
      "0      -0.684975 -0.424858 -0.377990  1.166370  0.156123  1.985740  1.271530   \n",
      "1      -0.585930 -0.205250  0.106778 -0.606237 -0.579892 -0.302930 -0.222930   \n",
      "2      -0.387839 -0.419209 -0.862757 -0.477787  0.524131 -0.302930 -0.222930   \n",
      "3       0.404524  2.716381 -0.741565 -0.151158 -0.579892  2.102114  5.007681   \n",
      "4       0.008342 -0.251502 -0.377990 -0.503477  0.524131 -0.302930 -0.222930   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "674995 -0.982112 -0.221138  1.439888 -0.492467  0.524131  2.050392  1.271530   \n",
      "674996 -0.982112 -0.424858  1.439888 -0.617247  0.156123 -0.302930 -0.222930   \n",
      "674997 -1.081157  0.005178  0.955121 -0.764047 -0.579892 -0.302930 -0.222930   \n",
      "674998  0.305478 -0.257857  1.803464 -0.657617 -0.211885 -0.302930 -0.222930   \n",
      "674999  1.394978 -0.424858 -1.347525 -0.598897 -0.579892 -0.302930 -0.222930   \n",
      "\n",
      "        job_admin.  job_blue-collar  job_entrepreneur  ...  month_jun  \\\n",
      "0         2.864015        -0.542198         -0.155463  ...  -0.377176   \n",
      "1        -0.349160         1.844345         -0.155463  ...  -0.377176   \n",
      "2        -0.349160        -0.542198         -0.155463  ...  -0.377176   \n",
      "3        -0.349160        -0.542198         -0.155463  ...  -0.377176   \n",
      "4        -0.349160        -0.542198         -0.155463  ...  -0.377176   \n",
      "...            ...              ...               ...  ...        ...   \n",
      "674995   -0.349160        -0.542198         -0.155463  ...  -0.377176   \n",
      "674996   -0.349160        -0.542198         -0.155463  ...  -0.377176   \n",
      "674997   -0.349160        -0.542198         -0.155463  ...  -0.377176   \n",
      "674998   -0.349160        -0.542198         -0.155463  ...  -0.377176   \n",
      "674999   -0.349160        -0.542198          6.432382  ...   2.651285   \n",
      "\n",
      "        month_mar  month_may  month_nov  month_oct  month_sep  \\\n",
      "0       -0.088162   1.510937  -0.310791  -0.111481  -0.099696   \n",
      "1       -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "2       -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "3       -0.088162  -0.661841  -0.310791  -0.111481  10.030489   \n",
      "4       -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "...           ...        ...        ...        ...        ...   \n",
      "674995  -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "674996  -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "674997  -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "674998  -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "674999  -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "\n",
      "        poutcome_failure  poutcome_other  poutcome_success  poutcome_unknown  \n",
      "0              -0.253027        7.054854         -0.155423         -2.943840  \n",
      "1              -0.253027       -0.141746         -0.155423          0.339692  \n",
      "2              -0.253027       -0.141746         -0.155423          0.339692  \n",
      "3               3.952148       -0.141746         -0.155423         -2.943840  \n",
      "4              -0.253027       -0.141746         -0.155423          0.339692  \n",
      "...                  ...             ...               ...               ...  \n",
      "674995          3.952148       -0.141746         -0.155423         -2.943840  \n",
      "674996         -0.253027       -0.141746         -0.155423          0.339692  \n",
      "674997         -0.253027       -0.141746         -0.155423          0.339692  \n",
      "674998         -0.253027       -0.141746         -0.155423          0.339692  \n",
      "674999         -0.253027       -0.141746         -0.155423          0.339692  \n",
      "\n",
      "[675000 rows x 51 columns]\n",
      "            age   balance       day  duration  campaign     pdays  previous  \\\n",
      "0      0.503569  0.026715  0.227969 -0.521827 -0.211885 -0.302930  -0.22293   \n",
      "1     -1.378293  0.458163 -0.377990  0.667251 -0.579892  1.067686   0.52430   \n",
      "2     -0.486885  0.053548  0.470353 -0.903507  0.524131 -0.302930  -0.22293   \n",
      "3      1.791159  0.110392 -1.589908 -0.679637 -0.211885 -0.302930  -0.22293   \n",
      "4     -0.189749  0.764979 -0.256798 -0.521827 -0.211885 -0.302930  -0.22293   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "74995  1.593068  0.601509  0.470353  1.768249 -0.579892 -0.302930  -0.22293   \n",
      "74996  0.503569  0.299989 -0.135606  0.839741 -0.211885 -0.302930  -0.22293   \n",
      "74997 -0.883066 -0.045664  0.470353 -0.888827  3.100184 -0.302930  -0.22293   \n",
      "74998  0.008342  0.508652  0.470353 -0.444757 -0.579892 -0.302930  -0.22293   \n",
      "74999  0.701660 -0.179829 -0.256798 -0.558527 -0.579892 -0.302930  -0.22293   \n",
      "\n",
      "       job_admin.  job_blue-collar  job_entrepreneur  ...  month_jun  \\\n",
      "0       -0.349160        -0.542198         -0.155463  ...  -0.377176   \n",
      "1        2.864015        -0.542198         -0.155463  ...  -0.377176   \n",
      "2       -0.349160        -0.542198         -0.155463  ...   2.651285   \n",
      "3       -0.349160        -0.542198         -0.155463  ...  -0.377176   \n",
      "4       -0.349160        -0.542198         -0.155463  ...  -0.377176   \n",
      "...           ...              ...               ...  ...        ...   \n",
      "74995    2.864015        -0.542198         -0.155463  ...  -0.377176   \n",
      "74996   -0.349160         1.844345         -0.155463  ...  -0.377176   \n",
      "74997   -0.349160        -0.542198         -0.155463  ...   2.651285   \n",
      "74998   -0.349160        -0.542198         -0.155463  ...  -0.377176   \n",
      "74999    2.864015        -0.542198         -0.155463  ...  -0.377176   \n",
      "\n",
      "       month_mar  month_may  month_nov  month_oct  month_sep  \\\n",
      "0      -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "1      -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "2      -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "3      -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "4      -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "...          ...        ...        ...        ...        ...   \n",
      "74995  -0.088162  -0.661841   3.217597  -0.111481  -0.099696   \n",
      "74996  -0.088162   1.510937  -0.310791  -0.111481  -0.099696   \n",
      "74997  -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "74998  -0.088162  -0.661841   3.217597  -0.111481  -0.099696   \n",
      "74999  -0.088162   1.510937  -0.310791  -0.111481  -0.099696   \n",
      "\n",
      "       poutcome_failure  poutcome_other  poutcome_success  poutcome_unknown  \n",
      "0             -0.253027       -0.141746         -0.155423          0.339692  \n",
      "1             -0.253027       -0.141746          6.434037         -2.943840  \n",
      "2             -0.253027       -0.141746         -0.155423          0.339692  \n",
      "3             -0.253027       -0.141746         -0.155423          0.339692  \n",
      "4             -0.253027       -0.141746         -0.155423          0.339692  \n",
      "...                 ...             ...               ...               ...  \n",
      "74995         -0.253027       -0.141746         -0.155423          0.339692  \n",
      "74996         -0.253027       -0.141746         -0.155423          0.339692  \n",
      "74997         -0.253027       -0.141746         -0.155423          0.339692  \n",
      "74998         -0.253027       -0.141746         -0.155423          0.339692  \n",
      "74999         -0.253027       -0.141746         -0.155423          0.339692  \n",
      "\n",
      "[75000 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "# Clean the dataset\n",
    "\n",
    "# Identify numerical columns (excluding the target and any non-feature columns)\n",
    "X_train_numerical_cols = X_train.select_dtypes(include=['int64', 'float64'])\n",
    "X_val_numerical_cols = X_val.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Identify categorical columns\n",
    "X_train_categorical_cols = X_train.select_dtypes(include=['object', 'category'])\n",
    "X_val_categorical_cols = X_val.select_dtypes(include=['object', 'category'])\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding using 1s and 0s\n",
    "X_train_categorical_encoded = pd.get_dummies(X_train_categorical_cols, drop_first=False).astype(int)\n",
    "X_val_categorical_encoded = pd.get_dummies(X_val_categorical_cols, drop_first=False).astype(int)\n",
    "\n",
    "# Combine the numerical and categorical features\n",
    "X_train_combined = pd.concat([X_train_numerical_cols, X_train_categorical_encoded], axis=1)\n",
    "X_val_combined = pd.concat([X_val_numerical_cols, X_val_categorical_encoded], axis=1)\n",
    "\n",
    "# Numeric Feature Standardization\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train_combined)\n",
    "X_val_scaled = sc.transform(X_val_combined)\n",
    "\n",
    "# Convert the scaled arrays back to DataFrames for easier handling\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train_combined.columns)\n",
    "X_val = pd.DataFrame(X_val_scaled, columns=X_val_combined.columns)\n",
    "\n",
    "# Show the cleaned dataset\n",
    "print(X_train)\n",
    "print(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "# Convert pandas DataFrames to Numpy arrays\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "# Builds tensors from numpy arrays\n",
    "x_train_tensor = torch.as_tensor(X_train).float()\n",
    "x_val_tensor = torch.as_tensor(X_val).float()\n",
    "# Reshapes y_train and y_val to be column vectors\n",
    "y_train_tensor = torch.as_tensor(y_train.reshape(-1, 1)).float()\n",
    "y_val_tensor = torch.as_tensor(y_val.reshape(-1, 1)).float()\n",
    "\n",
    "# Builds dataset containing ALL data points\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "# Builds a loader of each set\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "y = b + w_1x_1 + w_2x_2 + ... + w_nx_n + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large \n",
    "y =\n",
    "\\begin{cases}\n",
    "1,\\ \\text{if }b + w_1x_1 + w_2x_2 + ... + w_nx_n \\ge 0\n",
    "\\\\\n",
    "0,\\ \\text{if }b + w_1x_1 + w_2x_2 + ... + w_nx_n < 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "z = b + w_1x_1 + w_2x_2 + ... + w_nx_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\begin{aligned}\n",
    "& \\text{P}(y=1) \\approx 1.0, & \\text{if } &z \\gg 0\n",
    "\\\\\n",
    "& \\text{P}(y=1) = 0.5, & \\text{if } &z = 0\n",
    "\\\\\n",
    "& \\text{P}(y=1) \\approx 0.0, & \\text{if } &z \\ll 0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Logits to Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\begin{aligned}\n",
    "b + w_1x_1 + w_2x_2 + ... + w_nx_n = &\\ z = \\text{log}\\left(\\frac{p}{1-p}\\right) \\nonumber\n",
    "\\\\\n",
    "e^{b + w_1x_1 + w_2x_2 + ... + w_nx_n} = &\\ e^z = \\frac{p}{1-p} \\nonumber\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\begin{aligned}\n",
    "\\frac{1}{e^z}& = \\frac{1-p}{p}\n",
    "\\\\\n",
    "e^{-z}& = \\frac{1}{p} - 1\n",
    "\\\\\n",
    "1 + e^{-z}& = \\frac{1}{p}&\n",
    "\\\\\n",
    "p& = \\frac{1}{1 + e^{-z}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "p = \\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{P}(y=1) = \\sigma(z) = \\sigma(b+w_1x_1+w_2x_2 + ... + w_nx_n) = \\frac{1}{1+e^{-(b+w_1x_1+w_2x_2 + ... + w_nx_n)}}      \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Notation (for reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large W =\n",
    "\\underset{(3 \\times 1)}{\n",
    "\\begin{bmatrix}\n",
    "b \\\\\n",
    "w_1 \\\\\n",
    "w_2\n",
    "\\end{bmatrix}};\n",
    "X = \n",
    "\\underset{(3 \\times 1)}{\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large \n",
    "\\begin{aligned}\n",
    "z\n",
    "& = W^T X\n",
    "=\n",
    "\\underset{(1 \\times 3)}{\n",
    "\\begin{bmatrix}\n",
    "- & w^{T} & -\\\\\n",
    "\\end{bmatrix}}\n",
    "\\underset{(3 \\times 1)}{\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix}}\n",
    "= \\underset{(1 \\times 3)}{\n",
    "\\begin{bmatrix}\n",
    "b & w_1 & w_2\n",
    "\\end{bmatrix}}\n",
    "\\underset{(3 \\times 1)}{\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix}}\\\\\n",
    "& = b + w_1x_1 + w_2x_2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large y_i = 1 \\Rightarrow \\text{error}_i=\\text{log}(\\text{P}(y_i=1))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large \\text{P}(y_i=0)=1-\\text{P}(y_i=1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large y_i = 0 \\Rightarrow \\text{error}_i=\\text{log}(1-\\text{P}(y_i=1))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{BCE}(y)={-\\frac{1}{(N_{\\text{pos}}+N_{\\text{neg}})}\\Bigg[{\\sum_{i=1}^{N_{\\text{pos}}}{\\text{log}(\\text{P}(y_i=1))} + \\sum_{i=1}^{N_{\\text{neg}}}{\\text{log}(1 - \\text{P}(y_i=1))}}\\Bigg]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{BCE}(y)={-\\frac{1}{N}\\sum_{i=1}^{N}{\\left[y_i \\text{log}(\\text{P}(y_i=1)) + (1-y_i) \\text{log}(1-\\text{P}(y_i=1))\\right]}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 51\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.001\n",
    "\n",
    "torch.manual_seed(42)\n",
    "feature_cnt = X_train.shape[1]\n",
    "print(\"Number of features:\", feature_cnt)\n",
    "model = nn.Sequential()\n",
    "model.add_module('linear0', nn.Linear(feature_cnt, 10))\n",
    "model.add_module('activation0', nn.ReLU())\n",
    "model.add_module('linear2', nn.Linear(10, 3))\n",
    "model.add_module('activation2', nn.ReLU())\n",
    "model.add_module('output', nn.Linear(3, 1))\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines an Adam optimizer to update the parameters\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a BCE loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs = StepByStep(model=model, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 250\n",
    "\n",
    "sbs.set_loaders(train_loader, val_loader)\n",
    "sbs.set_tensorboard('run2', folder='binary_bank_comp/runs')\n",
    "sbs.train(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sbs.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "filename='binary_bank_comp/checkpoint_sgd_3layer_lr005_250.pth'\n",
    "sbs.save_checkpoint(filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id  age            job  marital  education default  balance housing  \\\n",
      "0  750000   32    blue-collar  married  secondary      no     1397     yes   \n",
      "1  750001   44     management  married   tertiary      no       23     yes   \n",
      "2  750002   36  self-employed  married    primary      no       46     yes   \n",
      "3  750003   58    blue-collar  married  secondary      no    -1380     yes   \n",
      "4  750004   28     technician   single  secondary      no     1950     yes   \n",
      "\n",
      "  loan   contact  day month  duration  campaign  pdays  previous poutcome  \n",
      "0   no   unknown   21   may       224         1     -1         0  unknown  \n",
      "1   no  cellular    3   apr       586         2     -1         0  unknown  \n",
      "2  yes  cellular   13   may       111         2     -1         0  unknown  \n",
      "3  yes   unknown   29   may       125         1     -1         0  unknown  \n",
      "4   no  cellular   22   jul       181         1     -1         0  unknown  \n"
     ]
    }
   ],
   "source": [
    "# Import data from .csv file\n",
    "raw_test_data = pd.read_csv('binary_bank_comp/dataset/test.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(raw_test_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove and save id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         750000\n",
       "1         750001\n",
       "2         750002\n",
       "3         750003\n",
       "4         750004\n",
       "           ...  \n",
       "249995    999995\n",
       "249996    999996\n",
       "249997    999997\n",
       "249998    999998\n",
       "249999    999999\n",
       "Name: id, Length: 250000, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove and save id column\n",
    "id_column = raw_test_data['id']\n",
    "raw_test_data.drop(columns=['id'], inplace=True)\n",
    "id_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             age   balance       day  duration  campaign     pdays  previous  \\\n",
      "0      -0.883066  0.068377  0.591545 -0.118128 -0.579892 -0.302930 -0.222930   \n",
      "1       0.305478 -0.416737 -1.589908  1.210410 -0.211885 -0.302930 -0.222930   \n",
      "2      -0.486885 -0.408617 -0.377990 -0.532837 -0.211885 -0.302930 -0.222930   \n",
      "3       1.692114 -0.912090  1.561080 -0.481457 -0.579892 -0.302930 -0.222930   \n",
      "4      -1.279248  0.263623  0.712737 -0.275938 -0.579892 -0.302930 -0.222930   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "249995  0.206433 -0.424858  0.227969 -0.701657 -0.211885 -0.302930 -0.222930   \n",
      "249996 -0.090703 -0.240557  0.349161  1.008560 -0.579892  2.153835  0.524300   \n",
      "249997  2.187341 -0.413207 -1.589908 -0.286948 -0.579892  0.899592  5.754912   \n",
      "249998  0.899751  0.503356  1.682272 -0.341998 -0.211885 -0.302930 -0.222930   \n",
      "249999 -1.180202 -0.169943 -1.226333 -0.503477 -0.579892 -0.302930 -0.222930   \n",
      "\n",
      "        job_admin.  job_blue-collar  job_entrepreneur  ...  month_jun  \\\n",
      "0         -0.34916         1.844345         -0.155463  ...  -0.377176   \n",
      "1         -0.34916        -0.542198         -0.155463  ...  -0.377176   \n",
      "2         -0.34916        -0.542198         -0.155463  ...  -0.377176   \n",
      "3         -0.34916         1.844345         -0.155463  ...  -0.377176   \n",
      "4         -0.34916        -0.542198         -0.155463  ...  -0.377176   \n",
      "...            ...              ...               ...  ...        ...   \n",
      "249995    -0.34916        -0.542198         -0.155463  ...  -0.377176   \n",
      "249996    -0.34916        -0.542198         -0.155463  ...  -0.377176   \n",
      "249997    -0.34916        -0.542198         -0.155463  ...  -0.377176   \n",
      "249998    -0.34916         1.844345         -0.155463  ...  -0.377176   \n",
      "249999    -0.34916        -0.542198         -0.155463  ...  -0.377176   \n",
      "\n",
      "        month_mar  month_may  month_nov  month_oct  month_sep  \\\n",
      "0       -0.088162   1.510937  -0.310791  -0.111481  -0.099696   \n",
      "1       -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "2       -0.088162   1.510937  -0.310791  -0.111481  -0.099696   \n",
      "3       -0.088162   1.510937  -0.310791  -0.111481  -0.099696   \n",
      "4       -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "...           ...        ...        ...        ...        ...   \n",
      "249995  -0.088162  -0.661841   3.217597  -0.111481  -0.099696   \n",
      "249996  -0.088162  -0.661841   3.217597  -0.111481  -0.099696   \n",
      "249997  -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "249998  -0.088162   1.510937  -0.310791  -0.111481  -0.099696   \n",
      "249999  -0.088162  -0.661841  -0.310791  -0.111481  -0.099696   \n",
      "\n",
      "        poutcome_failure  poutcome_other  poutcome_success  poutcome_unknown  \n",
      "0              -0.253027       -0.141746         -0.155423          0.339692  \n",
      "1              -0.253027       -0.141746         -0.155423          0.339692  \n",
      "2              -0.253027       -0.141746         -0.155423          0.339692  \n",
      "3              -0.253027       -0.141746         -0.155423          0.339692  \n",
      "4              -0.253027       -0.141746         -0.155423          0.339692  \n",
      "...                  ...             ...               ...               ...  \n",
      "249995         -0.253027       -0.141746         -0.155423          0.339692  \n",
      "249996          3.952148       -0.141746         -0.155423         -2.943840  \n",
      "249997         -0.253027       -0.141746          6.434037         -2.943840  \n",
      "249998         -0.253027       -0.141746         -0.155423          0.339692  \n",
      "249999         -0.253027       -0.141746         -0.155423          0.339692  \n",
      "\n",
      "[250000 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "# Clean the test dataset\n",
    "\n",
    "# Identify numerical columns (excluding the target and any non-feature columns)\n",
    "X_test_numerical_cols = raw_test_data.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Identify categorical columns\n",
    "X_test_categorical_cols = raw_test_data.select_dtypes(include=['object', 'category'])\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding using 1s and 0s\n",
    "X_test_categorical_encoded = pd.get_dummies(X_test_categorical_cols, drop_first=False).astype(int)\n",
    "\n",
    "# Combine the numerical and categorical features\n",
    "X_test_combined = pd.concat([X_test_numerical_cols, X_test_categorical_encoded], axis=1)\n",
    "\n",
    "# Numeric Feature Standardization\n",
    "# Note: sc = StandardScaler() from training data preprocessing fit on X_train_combined\n",
    "X_test_scaled = sc.transform(X_test_combined)\n",
    "\n",
    "# Convert the scaled arrays back to DataFrames for easier handling\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=X_test_combined.columns)\n",
    "\n",
    "# Show the cleaned dataset\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13) # Used for reproducibility of shuffle in DataLoader\n",
    "\n",
    "# Convert pandas DataFrames to Numpy arrays\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "# Builds tensors from numpy arrays\n",
    "x_test_tensor = torch.as_tensor(X_test).float()\n",
    "\n",
    "# Builds dataset containing ALL data points\n",
    "test_dataset = TensorDataset(x_test_tensor)\n",
    "\n",
    "# Builds a loader of each set\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration - defined in training section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition - defined in training section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model using filename defined in training section\n",
    "# filename='binary_bank_comp/checkpoint_sgd_3layer_lr005_250.pth'\n",
    "sbs.load_checkpoint(filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions (Logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.8138494],\n",
       "       [-1.3085092],\n",
       "       [-7.91028  ],\n",
       "       ...,\n",
       "       [ 1.1134808],\n",
       "       [-6.773426 ],\n",
       "       [-1.4504104]], shape=(250000, 1), dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = sbs.predict(x_test_tensor)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions (Probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0972511e-03],\n",
       "       [2.1273640e-01],\n",
       "       [3.6681708e-04],\n",
       "       ...,\n",
       "       [7.5277746e-01],\n",
       "       [1.1424626e-03],\n",
       "       [1.8993843e-01]], shape=(250000, 1), dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert predictions to probabilities using the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "probabilities = sigmoid(predictions)  # Use torch's built-in sigmoid for numerical stabilit\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions (Classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large y =\n",
    "\\begin{cases}\n",
    "1,\\ \\text{if P}(y=1) \\ge 0.5\n",
    "\\\\\n",
    "0,\\ \\text{if P}(y=1) < 0.5\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large y =\n",
    "\\begin{cases}\n",
    "1,\\ \\text{if } \\sigma(z) \\ge 0.5\n",
    "\\\\\n",
    "0,\\ \\text{if } \\sigma(z) < 0.5\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large y =\n",
    "\\begin{cases}\n",
    "1,\\ \\text{if } z \\ge 0\n",
    "\\\\\n",
    "0,\\ \\text{if } z < 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         750000\n",
      "1         750001\n",
      "2         750002\n",
      "3         750003\n",
      "4         750004\n",
      "           ...  \n",
      "249995    999995\n",
      "249996    999996\n",
      "249997    999997\n",
      "249998    999998\n",
      "249999    999999\n",
      "Name: id, Length: 250000, dtype: int64\n",
      "[0 0 0 ... 1 0 0]\n",
      "[1.09725108e-03 2.12736398e-01 3.66817083e-04 ... 7.52777457e-01\n",
      " 1.14246260e-03 1.89938426e-01]\n",
      "\n",
      "26559\n"
     ]
    }
   ],
   "source": [
    "classes = (predictions >= 0).astype(int)\n",
    "\n",
    "# Count positive predictions\n",
    "positive_count = (classes == 1).sum()\n",
    "\n",
    "output_classes = classes.astype(int).flatten()\n",
    "output_probabilities = probabilities.astype(float).flatten()\n",
    "\n",
    "# Get current date and time as a string, e.g., '2025-08-13_15-30-45'\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# Output index ids and classes to .csv file\n",
    "classes_df = pd.DataFrame({'id': id_column, 'class': output_classes})\n",
    "classes_filename = f'binary_bank_comp/results/predicted_classes_{timestamp}.csv'\n",
    "classes_df.to_csv(classes_filename, index=False)\n",
    "\n",
    "# Output index ids and probabilities to .csv file\n",
    "probabilities_df = pd.DataFrame({'id': id_column, 'y': output_probabilities})\n",
    "probabilities_filename = f'binary_bank_comp/results/predicted_probabilities_{timestamp}.csv'\n",
    "probabilities_df.to_csv(probabilities_filename, index=False)\n",
    "\n",
    "print(id_column)\n",
    "print(output_classes)\n",
    "print(output_probabilities)\n",
    "print()\n",
    "print(positive_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation - WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = (classes > 0).float()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_val = sbs.predict(X_val)\n",
    "probabilities_val = sigmoid(logits_val).squeeze()\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_line(ax, y, probs, threshold, shift=0.0, annot=False, colors=None):\n",
    "    if colors is None:\n",
    "        colors = ['r', 'b']\n",
    "    ax.grid(False)\n",
    "    ax.set_ylim([-.1, .1])\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.plot([0, 1], [0, 0], linewidth=2, c='k', zorder=1)\n",
    "    ax.plot([0, 0], [-.1, .1], c='k', zorder=1)\n",
    "    ax.plot([1, 1], [-.1, .1], c='k', zorder=1)\n",
    "\n",
    "    tn = (y == 0) & (probs < threshold)\n",
    "    fn = (y == 0) & (probs >= threshold)\n",
    "    tp = (y == 1) & (probs >= threshold)\n",
    "    fp = (y == 1) & (probs < threshold)\n",
    "\n",
    "    ax.plot([threshold, threshold], [-.1, .1], c='k', zorder=1, linestyle='--')\n",
    "    ax.scatter(probs[tn], np.zeros(tn.sum()) + shift, c=colors[0], s=150, zorder=2, edgecolor=colors[0], linewidth=3)\n",
    "    ax.scatter(probs[fn], np.zeros(fn.sum()) + shift, c=colors[0], s=150, zorder=2, edgecolor=colors[1], linewidth=3)\n",
    "\n",
    "    ax.scatter(probs[tp], np.zeros(tp.sum()) - shift, c=colors[1], s=150, zorder=2, edgecolor=colors[1], linewidth=3)\n",
    "    ax.scatter(probs[fp], np.zeros(fp.sum()) - shift, c=colors[1], s=150, zorder=2, edgecolor=colors[0], linewidth=3)\n",
    "\n",
    "    ax.set_xlabel(r'$\\sigma(z) = P(y=1)$')\n",
    "    ax.set_title('Threshold = {}'.format(threshold))\n",
    "\n",
    "    if annot:\n",
    "        ax.annotate('TN', xy=(.20, .03), c='k', weight='bold', fontsize=20)\n",
    "        ax.annotate('FN', xy=(.20, -.08), c='k', weight='bold', fontsize=20)\n",
    "        ax.annotate('FP', xy=(.70, .03), c='k', weight='bold', fontsize=20)\n",
    "        ax.annotate('TP', xy=(.70, -.08), c='k', weight='bold', fontsize=20)\n",
    "    return ax\n",
    "\n",
    "def probability_contour(ax, model, device, X, y, threshold, cm=None, cm_bright=None):\n",
    "    if cm is None:\n",
    "        cm = plt.cm.RdBu\n",
    "    if cm_bright is None:\n",
    "        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "    h = .02  # step size in the mesh\n",
    "\n",
    "    x_min, x_max = -2.25, 2.25\n",
    "    y_min, y_max = -2.25, 2.25\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    logits = model(torch.as_tensor(np.c_[xx.ravel(), yy.ravel()]).float().to(device))\n",
    "    logits = logits.detach().cpu().numpy().reshape(xx.shape)\n",
    "\n",
    "    yhat = sigmoid(logits)\n",
    "\n",
    "    ax.contour(xx, yy, yhat, levels=[threshold], cmap=\"Greys\", vmin=0, vmax=1)\n",
    "    contour = ax.contourf(xx, yy, yhat, 25, cmap=cm, alpha=.8, vmin=0, vmax=1)\n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, edgecolors='k')\n",
    "    # Plot the testing points\n",
    "    #ax.scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap=cm_bright, edgecolors='k', alpha=0.6)\n",
    "\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel(r'$X_1$')\n",
    "    ax.set_ylabel(r'$X_2$')\n",
    "    ax.set_title(r'$\\sigma(z) = P(y=1)$')\n",
    "    ax.grid(False)\n",
    "\n",
    "    ax_c = plt.colorbar(contour)\n",
    "    ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "    return ax\n",
    "\n",
    "def figure9(x, y, model, device, probabilities, threshold, shift=0.0, annot=False, cm=None, cm_bright=None):\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    gs = fig.add_gridspec(3, 3)\n",
    "\n",
    "    ax = fig.add_subplot(gs[:, 0])\n",
    "    probability_contour(ax, model, device, x, y, threshold, cm, cm_bright)\n",
    "    \n",
    "    if cm_bright is None:\n",
    "        colors = ['r', 'b']\n",
    "    else:\n",
    "        colors = cm_bright.colors\n",
    "\n",
    "    ax = fig.add_subplot(gs[1, 1:])\n",
    "    probability_line(ax, y, probabilities, threshold, shift, annot, colors)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = figure9(X_val, y_val, sbs.model, sbs.device, probabilities_val, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure10(y_val, probabilities_val, threshold, 0.04, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure10(y_val, probabilities_val, threshold, 0.04, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_thresh50 = confusion_matrix(y_val, (probabilities_val >= .5))\n",
    "cm_thresh50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True and False Positives and Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cm(cm):\n",
    "    # Actual negatives go in the top row, \n",
    "    # above the probability line\n",
    "    actual_negative = cm[0]\n",
    "    # Predicted negatives go in the first column\n",
    "    tn = actual_negative[0]\n",
    "    # Predicted positives go in the second column\n",
    "    fp = actual_negative[1]\n",
    "\n",
    "    # Actual positives go in the bottow row, \n",
    "    # below the probability line\n",
    "    actual_positive = cm[1]\n",
    "    # Predicted negatives go in the first column\n",
    "    fn = actual_positive[0]\n",
    "    # Predicted positives go in the second column\n",
    "    tp = actual_positive[1]\n",
    "    \n",
    "    return tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True and False Positive Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large \\text{TPR} = \\frac{\\text{TP}}{\\text{TP + FN}} \\ \\ \\  \\text{FPR} = \\frac{\\text{FP}}{\\text{FP + TN}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr_fpr(cm):\n",
    "    tn, fp, fn, tp = split_cm(cm)\n",
    "    \n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "    \n",
    "    return tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_fpr(cm_thresh50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large \\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}} \\ \\ \\  \\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(cm):\n",
    "    tn, fp, fn, tp = split_cm(cm)\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall(cm_thresh50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large \\text{Accuracy} = \\frac{\\text{TP+TN}}{\\text{TP+TN+FP+FN}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade-offs and Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = eval_curves_from_probs(y_val, probabilities_val, [.5], annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure9(X_val, y_val, sbs.model, sbs.device, probabilities_val, threshold=0.3, shift=0.04, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_val, (probabilities_val >= 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = eval_curves_from_probs(y_val, probabilities_val, [.3, .5], annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure9(X_val, y_val, sbs.model, sbs.device, probabilities_val, threshold=0.7, shift=0.04, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_val, (probabilities_val >= 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = eval_curves_from_probs(y_val, probabilities_val, [.3, .5, .7], annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC and PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshs = np.linspace(0.,1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure17(y_val, probabilities_val, threshs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds1 = roc_curve(y_val, probabilities_val)\n",
    "prec, rec, thresholds2 = precision_recall_curve(y_val, probabilities_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = eval_curves(fpr, tpr, rec, prec, thresholds1, thresholds2, line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Precision Quirk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure19(y_val, probabilities_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large \\text{Precision}(\\text{thresh}=0.40)=\\frac{13}{13+2}=0.8666\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large \\text{Precision}(\\text{thresh}=0.50)=\\frac{(13-1)}{(13-1)+2}=\\frac{12}{12+2}=0.8571\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large \\text{Precision}(\\text{thresh}=0.57)=\\frac{12}{12+(2-1)}=\\frac{12}{12+1}=0.9230\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best and Worst Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best\n",
    "fig = figure20(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(39)\n",
    "random_probs = np.random.uniform(size=y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_random, tpr_random, thresholds1_random = roc_curve(y_val, random_probs)\n",
    "prec_random, rec_random, thresholds2_random = precision_recall_curve(y_val, random_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worst\n",
    "fig = figure21(y_val, random_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area under the curves of our model\n",
    "auroc = auc(fpr, tpr)\n",
    "aupr = auc(rec, prec)\n",
    "print(auroc, aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area under the curves of the random model\n",
    "auroc_random = auc(fpr_random, tpr_random)\n",
    "aupr_random = auc(rec_random, prec_random)\n",
    "print(auroc_random, aupr_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "# Builds tensors from numpy arrays\n",
    "x_train_tensor = torch.as_tensor(X_train).float()\n",
    "y_train_tensor = torch.as_tensor(y_train.reshape(-1, 1)).float()\n",
    "\n",
    "x_val_tensor = torch.as_tensor(X_val).float()\n",
    "y_val_tensor = torch.as_tensor(y_val.reshape(-1, 1)).float()\n",
    "\n",
    "# Builds dataset containing ALL data points\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "# Builds a loader of each set\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = nn.Sequential()\n",
    "model.add_module('linear', nn.Linear(2, 1))\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a BCE loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "sbs = StepByStep(model, loss_fn, optimizer)\n",
    "sbs.set_loaders(train_loader, val_loader)\n",
    "sbs.train(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_val = sbs.predict(X_val)\n",
    "probabilities_val = sigmoid(logits_val).squeeze()\n",
    "cm_thresh50 = confusion_matrix(y_val, (probabilities_val >= 0.5))\n",
    "cm_thresh50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
